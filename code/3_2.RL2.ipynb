{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, yaml, wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from rl import make_transition, make_transition_for_AKI, imvt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "if os.getcwd()[-4:] == \"code\":\n",
    "    os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"./code/params.yaml\")) as f:\n",
    "        params = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchanreverse\u001b[0m (\u001b[33mdahs\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\DAHS\\Documents\\GitHub\\AIAKI\\wandb\\run-20240214_213447-hk99a7t0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dahs/AIAKI/runs/hk99a7t0' target=\"_blank\">IMV_LSTM_Rescue_mse</a></strong> to <a href='https://wandb.ai/dahs/AIAKI' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dahs/AIAKI' target=\"_blank\">https://wandb.ai/dahs/AIAKI</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dahs/AIAKI/runs/hk99a7t0' target=\"_blank\">https://wandb.ai/dahs/AIAKI/runs/hk99a7t0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dahs/AIAKI/runs/hk99a7t0?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1d0d4880490>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    name=\"IMV_LSTM_Rescue_mse\",\n",
    "    project=\"AIAKI\",\n",
    "    config={\n",
    "        \"learning_rate\":params[\"learning_rate\"],\n",
    "        \"epoch\":params['epoch'],\n",
    "        \"batch_size\":params[\"minibatch_size\"],\n",
    "        \"n_units\":params[\"n_units\"],\n",
    "        \"update_freq\":params[\"update_freq\"]\n",
    "    }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11426/11426 [00:33<00:00, 336.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished train data transition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 762/762 [00:01<00:00, 560.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished val or test data trnsition\n"
     ]
    }
   ],
   "source": [
    "train_loader, train_len = make_transition(params['train'],rolling_size=24,batch_size=64,istrain=True)\n",
    "val_loader, val_len = make_transition(params['val'],rolling_size=24,batch_size=256,istrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "network = imvt(input_dim=params['state_dim'], output_dim=params['num_actions'], n_units=params['n_units'], device=device).to(device)\n",
    "target_network = imvt(input_dim=params['state_dim'], output_dim=params['num_actions'], n_units=params['n_units'], device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 8674/15689 [08:42<06:53, 16.95it/s]"
     ]
    }
   ],
   "source": [
    "epoch = params['epoch']\n",
    "gamma = params['gamma']\n",
    "optimizer = optim.Adam(network.parameters(), lr=params['learning_rate'], amsgrad=True)\n",
    "\n",
    "update_freq = params['update_freq']\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "\n",
    "for i in range(epoch):\n",
    "    loss_train = 0\n",
    "    update_counter = 0\n",
    "    for s,a,r1,r2,r3,s2,t in tqdm(train_loader):\n",
    "        s = s.to(device)\n",
    "        a = a.to(device)\n",
    "        r1 = r1.to(device)\n",
    "        r2 = r2.to(device)\n",
    "        r3 = r3.to(device)\n",
    "        s2 = s2.to(device)\n",
    "        t = t.to(device)\n",
    "\n",
    "        q,_,_ = network(s)\n",
    "        q_pred = q.gather(1, a).squeeze()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q2,_,_ = target_network(s2)\n",
    "            q2_net, _, _ = network(s2)\n",
    "\n",
    "        q2_max = q2.gather(1, torch.max(q2_net,dim=1)[1].unsqueeze(1)).squeeze(1).detach()\n",
    "\n",
    "        bellman_target = torch.clamp(r2, max=1.0, min=0.0) + gamma * torch.clamp(q2_max.detach(), max=1.0, min=0.0)*(1-t)\n",
    "        loss = F.mse_loss(q_pred, bellman_target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        loss_train += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        update_counter += 1\n",
    "        if update_counter == update_freq:\n",
    "            target_network.load_state_dict(network.state_dict())\n",
    "            update_counter = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss_val = 0\n",
    "\n",
    "        for s,a,r1,r2,r3,s2,t in val_loader:\n",
    "            s = s.to(device)\n",
    "            a = a.to(device)\n",
    "            r1 = r1.to(device)\n",
    "            r2 = r2.to(device)\n",
    "            r3 = r3.to(device)\n",
    "            s2 = s2.to(device)\n",
    "            t = t.to(device)\n",
    "\n",
    "            q,_,_ = network(s)\n",
    "            q2,_,_ = target_network(s2)\n",
    "            q2 = q2.detach()\n",
    "            q_pred = q.gather(1, a).squeeze()\n",
    "\n",
    "            q2_net,_,_ = network(s2)\n",
    "            q2_net = q2_net.detach()\n",
    "            q2_max = q2.gather(1, torch.max(q2_net,dim=1)[1].unsqueeze(1)).squeeze()\n",
    "\n",
    "            bellman_target = torch.clamp(r2, max=1.0, min=0.0) + gamma * torch.clamp(q2_max.detach(), max=1.0, min=0.0)*(1-t)\n",
    "            loss = F.mse_loss(q_pred, bellman_target)\n",
    "            loss_val += loss.item()\n",
    "            \n",
    "    wandb.log({\"Iter:\": i, \"train:\":loss_train/train_len, \"val:\":loss_val/val_len})\n",
    "    print(\"Iter:\", i, \"train:\", loss_train/train_len, \"val:\",loss_val/val_len)\n",
    "    torch.save(network.state_dict(), './checkpoint/checkpoint_%i.pt'%i)\n",
    "    train_loss_history.append(loss_train/train_len)\n",
    "    val_loss_history.append(loss_val/val_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
